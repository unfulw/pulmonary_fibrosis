{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f9409a",
   "metadata": {},
   "source": [
    "### Import Libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a9a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import nibabel as nib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9de075",
   "metadata": {},
   "source": [
    "### Load Image Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c82d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads a single dicom image and converts it to a numpy array\n",
    "def load_dicom_image(file_path):\n",
    "    \"\"\"Load a DICOM image and return it as a numpy array.\"\"\"\n",
    "    dicom = pydicom.dcmread(file_path)\n",
    "    image = dicom.pixel_array\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image, target_size=(256, 256)):\n",
    "    \"\"\"Preprocess the image: resize and normalize.\"\"\"\n",
    "    image_resized = cv2.resize(image, target_size)\n",
    "    image_normalized = (image_resized - np.min(image_resized)) / (np.max(image_resized) - np.min(image_resized))\n",
    "    return image_normalized\n",
    "\n",
    "# reads all dicom images from a directory and preprocesses them\n",
    "def load_dicom_images_from_directory(directory):\n",
    "    \"\"\"Load and preprocess all DICOM images from a directory.\"\"\"\n",
    "    file_paths = glob(os.path.join(directory, '*.dcm'))\n",
    "    images = []\n",
    "    for file_path in file_paths:\n",
    "        image = load_dicom_image(file_path)\n",
    "        image_preprocessed = preprocess_image(image)\n",
    "        images.append(image_preprocessed)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient: ID00184637202242062969203\n",
      "Found 62 DICOM slices\n"
     ]
    }
   ],
   "source": [
    "# testing the functions (IGNORE THIS)\n",
    "\n",
    "dicom_files = glob(os.path.join('path_to_dicom_files', '*.dcm'))\n",
    "for file in dicom_files:\n",
    "    img = load_dicom_image(file)\n",
    "    img_preprocessed = preprocess_image(img)\n",
    "    print(f\"Processed image shape: {img_preprocessed.shape}\")\n",
    "    break  # Just process one file for demonstration\n",
    "\n",
    "# --- PATH TO YOUR DATA ---\n",
    "root_folder = \"/Users/jinkyungjeon/cs3244_project/osic-pulmonary-fibrosis-progression/train\"\n",
    "\n",
    "# Choose one patient\n",
    "patient_id = \"ID00184637202242062969203\"  # change this to any patient folder name\n",
    "patient_folder = os.path.join(root_folder, patient_id)\n",
    "\n",
    "# Get all DICOM file paths in that patient's folder\n",
    "dicom_files = sorted([os.path.join(patient_folder, f) for f in os.listdir(patient_folder) if f.endswith(\".dcm\")])\n",
    "\n",
    "print(f\"Patient: {patient_id}\")\n",
    "print(f\"Found {len(dicom_files)} DICOM slices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929ceb1",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicom_to_hu(path: str) -> np.ndarray:\n",
    "    \"\"\"Convert DICOM pixel data to Hounsfield Units (HU).\"\"\"\n",
    "    dicom = pydicom.dcmread(path)\n",
    "    image = dicom.pixel_array.astype(np.int16)\n",
    "\n",
    "    # Convert to HU\n",
    "    intercept = dicom.RescaleIntercept\n",
    "    slope = dicom.RescaleSlope \n",
    "\n",
    "    image = slope * image + intercept\n",
    "\n",
    "    return image\n",
    "\n",
    "def lung_window(img_hu: np.ndarray, center = -600, width = 1500) -> np.ndarray:\n",
    "    \"\"\"Apply lung windowing to the HU image.\"\"\"\n",
    "\n",
    "    min_hu = center - (width // 2)\n",
    "    max_hu = center + (width // 2)\n",
    "\n",
    "    img_windowed = np.clip(img_hu, min_hu, max_hu) \n",
    "\n",
    "    return img_windowed\n",
    "\n",
    "def load_patient_stack(dicom_dir: str, target_depth: int = 96, img_size: int = 224) -> np.ndarray:\n",
    "    \"\"\"Load all slices, sort by InstanceNumber, window, resize, depth-pad/trim.\"\"\"\n",
    "    files = glob.glob(os.path.join(dicom_dir, \"*.dcm\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No DICOMs in {dicom_dir}\")\n",
    "\n",
    "    # sort slices using InstanceNumber or filename fallback\n",
    "    def inst_no(fp):\n",
    "        try:\n",
    "            return int(pydicom.dcmread(fp, stop_before_pixels=True).InstanceNumber)\n",
    "        except Exception:\n",
    "            return 0\n",
    "    files.sort(key=inst_no)\n",
    "\n",
    "    slices = [lung_window(dicom_to_hu(fp)) for fp in files]\n",
    "\n",
    "    # resize each slice\n",
    "    slices = [cv2.resize(s, (img_size, img_size), interpolation=cv2.INTER_AREA) for s in slices]\n",
    "    vol = np.stack(slices, axis=0)  # [D, H, W]\n",
    "\n",
    "    # normalize per-volume (optional but helpful)\n",
    "    v = vol.astype(np.float32)\n",
    "    v = (v - v.mean()) / (v.std() + 1e-6)\n",
    "\n",
    "    # depth pad/trim to target_depth\n",
    "    D = v.shape[0]\n",
    "    if D == target_depth:\n",
    "        pass\n",
    "    elif D > target_depth:\n",
    "        # uniform downsample to target_depth\n",
    "        idx = np.linspace(0, D-1, target_depth).round().astype(int)\n",
    "        v = v[idx]\n",
    "    else:\n",
    "        # pad by symmetric reflection\n",
    "        pad_needed = target_depth - D\n",
    "        pre = pad_needed // 2\n",
    "        post = pad_needed - pre\n",
    "        v = np.pad(v, ((pre, post), (0,0), (0,0)), mode=\"reflect\")\n",
    "\n",
    "    # channel-first for 2D CNN over slices: keep as [D, H, W]; weâ€™ll add channel later\n",
    "    return v  # float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a92255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "ct_image = nib.load(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca26b45",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/Users/jinkyungjeon/cs3244_project/osic-pulmonary-fibrosis-progression\"\n",
    "train_csv = pd.read_csv(os.path.join(dir, 'train.csv'))\n",
    "train_path = os.path.join(dir, 'train')\n",
    "\n",
    "train_path = os.path.join(dir, 'train')\n",
    "test_path = os.path.join(dir, 'test')\n",
    "\n",
    "train_patients = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "test_patients = [d for d in os.listdir(test_path) if os.path.isdir(os.path.join(test_path, d))]\n",
    "\n",
    "print(len(train_patients), len(test_patients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46896641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of patients in the training tabular file \n",
    "train_csv = pd.read_csv(os.path.join(dir, 'train.csv'))\n",
    "print(len(train_csv)) # prints the number of rows in the CSV file\n",
    "\n",
    "# Number of patients in the training ct scan folder\n",
    "train_patients = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
    "print(len(train_patients)) # prints the number of patient folders in the training folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f597a5",
   "metadata": {},
   "source": [
    "### Simple CNN Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "341bbc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs3244/lib/python3.9/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.4 when it was built against 1.14.3, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn(input_shape=(224,224,1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "    return models.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ef241",
   "metadata": {},
   "source": [
    "### Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/cs3244/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X_train = load_dicom_images_from_directory(os.path.join(train_path, patient_id))\n",
    "y_train = train_csv[train_csv['Patient'] == patient_id]['FVC'].values\n",
    "\n",
    "X_test = load_dicom_images_from_directory(os.path.join(test_path, patient_id))\n",
    "y_test = train_csv[train_csv['Patient'] == patient_id]['FVC'].values\n",
    "\n",
    "# Build CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(256,256,1)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1, activation='linear')  # regression for FVC\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "\n",
    "# Suppose X_train and y_train are ready\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs3244",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
